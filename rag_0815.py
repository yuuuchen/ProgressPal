# -*- coding: utf-8 -*-
"""rag_0815.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14jAxpxWKDhlFS2xe0hTOhfQiZ7txP5-P
"""


from langchain_community.document_loaders import PyPDFLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter,MarkdownHeaderTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import ConversationalRetrievalChain
from sentence_transformers import SentenceTransformer
from langchain.schema import Document
import re
from rank_bm25 import BM25Okapi
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import os
import jieba

"""測試

# **教材向量庫**

建立向量庫
"""

#讀取資料夾裡的所有.md檔案
md_files = [f for f in os.listdir("/content/drive/MyDrive/專題/教材") if f.endswith(".md")]

all_docs = []

#定義標題層級
headers_to_split_on = [
    ("#", "章節"),
    ("##", "單元"),
    ("###", "段落"),
    ("####", "子段落")
]
markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=20,
    length_function=len
)

for file in md_files:
  file_path = os.path.join("/content/drive/MyDrive/專題/教材", file)

  #讀取檔案
  loader = TextLoader(file_path, encoding="utf-8")
  docs = loader.load()

  for d in docs:
    #MarkdownHeaderTextSplitter分段
    md_header_splits = markdown_splitter.split_text(d.page_content)

    for doc in md_header_splits:
      header = doc.metadata.get("段落", "")
      content_with_header = f"{header}\n{doc.page_content}" if header else doc.page_content

      all_docs.append(
        Document(
          page_content=content_with_header,
          metadata={
              **doc.metadata,   # 保留章節、小節資訊
              "source": file    # 加上檔名來源
          }
        )
      )

print(f"總共有 {len(all_docs)} 個段落，來自 {len(md_files)} 個 Markdown 檔案")

#將文本轉為向量
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

#存入 Chroma 向量資料庫
vectorstore = Chroma.from_documents(
    documents=all_docs,
    embedding=embeddings,
    persist_directory="/content/drive/MyDrive/專題/教材/teaching_material"
)

vectorstore.persist()

for i, doc in enumerate(all_docs[:70], 1):
    print(f"--- 段落 {i} ---")
    print(doc.page_content.strip())   # 段落文字
    print("Metadata:", doc.metadata)  # 對應的標題資訊（章節/單元/來源檔案）
    print()

"""**回傳教材內容**"""

#排序單元編號
def parse_unit_code(unit_code):
    parts = unit_code.split("-")
    nums = []
    for p in parts[:2]:  # 只取前兩項：章節與單元
        match = re.match(r"(\d+)", p)
        if match:
            nums.append(int(match.group(1)))
        else:
            nums.append(0)
    return nums  #回傳一個數字列表，例如 "1-1-1" → [1,1]

#依據章節與單元號碼，輸出該單元的教材內容字串
def get_unit(chapter, unit):
  docs_dict = {}

  for doc in all_docs:
    text = doc.page_content.strip()
    meta = doc.metadata

    unit_title = meta.get("單元", "")
    paragraph = meta.get("段落", "")
    if unit_title:
        unit_num = re.match(r"(\d+-\d+)", unit_title).group(0)  # 抓單元開頭數字

        if unit_num not in docs_dict:
            docs_dict[unit_num] = []

        # 在內容中保留段落資訊
        docs_dict[unit_num].append(f"{text}")

  target_code = f"{chapter}-{unit}" #組合成目標單元編號

  combined = []
  for key in sorted(docs_dict.keys(), key=parse_unit_code):
    if key == target_code:
      combined.append(f"=== {key} ===")
      combined.extend(docs_dict[key])

  if combined:
    return "\n".join(combined)
  else:
    return None

#依據章節，輸出該單元的教材內容字串
def get_chapter(chapter):
  docs_dict = {}

  for doc in all_docs:
      text = doc.page_content.strip()
      meta = doc.metadata

      unit_title = meta.get("單元", "")
      if unit_title:
          unit_num = re.match(r"(\d+-\d+)", unit_title).group(0)  # 抓單元編號

          if unit_num not in docs_dict:
              docs_dict[unit_num] = []

          # 在內容中保留原文
          docs_dict[unit_num].append(text)

  # 找出所有屬於該章節的單元
  combined = []
  for key in sorted(docs_dict.keys(), key=parse_unit_code):
      if key.startswith(f"{chapter}-"):  # 判斷章節開頭
          combined.append(f"=== {key} ===")
          combined.extend(docs_dict[key])

  if combined:
      return "\n".join(combined)
  else:
      return None

"""測試"""

print(get_unit(1,3))

print(get_chapter(4))

"""**Chroma + BM25 混合搜尋**"""

def retrieve_docs(query, top_k=5, weight_bm25=0.7, weight_vector=0.3):
  # 載入Embeddings和Chroma
  embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
  vectorstore = Chroma(
      persist_directory="/content/drive/MyDrive/專題/程式碼專區/teaching_material",
      embedding_function=embeddings
  )

  #取得關鍵字
  keywords = query.get("keywords", [])
  results = []  # 最終結果

  #遍歷每個關鍵字
  for keyword in keywords:
    #先在metadata章節中過濾出包含該關鍵字的段落
    #沒有符合就改用全教材搜尋
    filtered_docs = [doc for doc in all_docs if keyword in doc.metadata.get("章節", "")]
    corpus_docs = filtered_docs if filtered_docs else all_docs
    corpus_texts = [doc.page_content for doc in corpus_docs]

    #BM25準備，jieba做全模式分詞
    tokenized_corpus = [list(jieba.cut(text, cut_all=True)) for text in corpus_texts]
    #建立BM25模型
    bm25 = BM25Okapi(tokenized_corpus)
    query_tokens = list(jieba.cut(keyword, cut_all=True))
    bm25_scores = bm25.get_scores(query_tokens)

    #排序候選文件
    bm25_ranked = sorted(
        zip(bm25_scores, corpus_texts),
        key=lambda x: x[0],
        reverse=True
    )

    #metadata篩選
    if filtered_docs:
      candidate_pairs = bm25_ranked[:10]  #章節內取前10
    #全教材搜尋
    else:
      candidate_pairs = [(score, doc) for score, doc in bm25_ranked if score > 0][:10] #過濾出score>0，再取前10

    #如果沒有任何候選，就跳到下一個keyword
    if not candidate_pairs:
      continue

    candidate_docs = [doc for _, doc in candidate_pairs]
    bm25_scores_candidates = np.array([score for score, _ in candidate_pairs])

    #向量檢索
    query_emb = embeddings.embed_query(keyword)
    vector_scores = []
    for doc in candidate_docs:
        doc_emb = embeddings.embed_documents([doc])[0]
        score = np.dot(query_emb, doc_emb) / (
            np.linalg.norm(query_emb) * np.linalg.norm(doc_emb)
        )
        vector_scores.append(score)
    vector_scores = np.array(vector_scores)

    #標準化分數
    def normalize(arr):
      if len(arr) > 1:
        return (arr - np.min(arr)) / (np.ptp(arr) + 1e-9)
      else:
        return np.array([1.0])  # 單一元素直接給 1

    bm25_scores_norm = normalize(bm25_scores_candidates)
    vector_scores_norm = normalize(vector_scores)

    #融合分數
    final_scores = weight_bm25 * bm25_scores_norm + weight_vector * vector_scores_norm

    #排序/取前 top_k
    sorted_pairs = sorted(
        zip(final_scores, candidate_docs),
        key=lambda x: x[0],
        reverse=True
    )
    top_docs = [doc for _, doc in sorted_pairs[:top_k]]

    #去除重複段落
    for doc in top_docs:
      if doc not in results:
        results.append(doc)

  return results if results else None  #回傳結果list，[]為空則回傳None

query = {"keywords": ["鏈結串列","索引值"]}
related_docs = retrieve_docs(query,5)

print("找到的相關教材段落：")
for i, doc in enumerate(related_docs, 1):
  print(f"{i}. {doc}\n")
