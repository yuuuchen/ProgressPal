# -*- coding: utf-8 -*-
"""rag_0815.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14jAxpxWKDhlFS2xe0hTOhfQiZ7txP5-P
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -U langchain-community langchain-openai langchain-chroma langchain-qdrant langchain-google-vertexai -q

!pip install pypdf -q

!pip install -U sentence-transformers -q

from langchain_community.document_loaders import PyPDFLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter,MarkdownHeaderTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import ConversationalRetrievalChain
from sentence_transformers import SentenceTransformer
from langchain.schema import Document
import re

!pip install rank_bm25 -q

"""# RecursiveCharacterTextSplitter方法讀取教材(/chroma_db)

Chunk 切分
"""

#載入教材PDF
loader = PyPDFLoader("/content/drive/MyDrive/專題/教材/CH3陣列_整理版.pdf")
documents = loader.load()
#切分文字
text_splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=50)
docs = text_splitter.split_documents(documents)

print(f"共分成 {len(docs)} 段")

# for i, chunk in enumerate(docs, 1):
#   print(f"段落 {i}:\n{chunk.page_content}\n{'-'*50}")

"""Embeddings"""

#將文本轉成向量
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

"""用Chroma建立向量庫"""

vectorstore = Chroma.from_documents(docs, embeddings, persist_directory="/content/drive/MyDrive/專題/程式碼專區/chroma_db")
vectorstore.persist()

"""將使用者問題轉成向量，找教材裡最相似的幾段內容

**Chroma**
"""

def retrieve_docs_rc(query, top_k):
  #載入Embeddings和Chroma向量庫
  embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
  vectorstore = Chroma(persist_directory="/content/drive/MyDrive/專題/程式碼專區/chroma_db", embedding_function=embeddings)

  results = vectorstore.similarity_search_with_score(query, k=top_k)
  related_texts = []

  for doc, score in results:
    if score <= 1:
      related_texts.append((doc.page_content))

  return related_texts #回傳成一個list

"""測試"""

query = "一維陣列"
related_docs = retrieve_docs_rc(query, top_k=5)

print("找到的相關教材段落：")
for i, doc in enumerate(related_docs, 1):
  print(f"{i}. {doc}\n")

"""**Chroma + BM25 混合搜尋**"""

def hybrid_search_rc(query,top_k=5, weight_bm25=0.7, weight_vector=0.3):
    #載入Embeddings和Chroma向量庫
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vectorstore = Chroma(
      persist_directory="/content/drive/MyDrive/專題/程式碼專區/chroma_db",
      embedding_function=embeddings
    )

    #抓取所有文件（for BM25）
    all_docs = vectorstore.get(include=["documents"])["documents"]

    #準備BM25資料
    #把每份文件用split拆成單詞
    tokenized_corpus = [doc.split() for doc in all_docs]
    bm25 = BM25Okapi(tokenized_corpus)

    #BM25排序
    bm25_scores = bm25.get_scores(query.split())
    bm25_ranked = sorted(zip(bm25_scores, all_docs), key=lambda x: x[0], reverse=True)

    #取前20筆作為候選
    candidate_docs = [doc for _, doc in bm25_ranked[:20]]

    #做向量檢索
    query_emb = embeddings.embed_query(query) #把 query 轉成向量
    vector_results = []
    for doc in candidate_docs:
        doc_emb = embeddings.embed_query(doc)
        #餘弦相似度（越大越相似）
        score = np.dot(query_emb, doc_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(doc_emb))
        vector_results.append((doc, score))

    #取出BM25 數與向量分數
    bm25_scores_candidates = np.array([score for score, _ in bm25_ranked[:20]])
    vector_scores = np.array([score for _, score in vector_results])

    #標準化
    scaler = MinMaxScaler()
    bm25_scores_norm = scaler.fit_transform(bm25_scores_candidates.reshape(-1, 1)).flatten()
    vector_scores_norm = scaler.fit_transform(vector_scores.reshape(-1, 1)).flatten()

    #融合分數(70%來自BM25/30%來自向量相似度)
    final_scores = weight_bm25 * bm25_scores_norm + weight_vector * vector_scores_norm

    #排序
    sorted_pairs = sorted(zip(final_scores, candidate_docs), key=lambda x: x[0], reverse=True)
    sorted_results = [text for _, text in sorted_pairs]

    return sorted_results[:top_k]

"""測試"""

query = "一維陣列"
related_docs = hybrid_search_rc(query,top_k=5)

print("找到的相關教材段落：")
for i, doc in enumerate(related_docs, 1):
  print(f"{i}. {doc}\n")

"""# MarkdownHeaderTextSplitter方法讀取教材(/chroma_db_md)

Chunk 切分
"""

#載入教材PDF
loader = PyPDFLoader("/content/drive/MyDrive/專題/教材/CH3陣列_markdown.pdf")
pages = loader.load()

# 合併文字
text = "\n".join([p.page_content for p in pages])

# 定義標題層級
headers_to_split_on = [
    ("#", "章節"),
    ("##", "小節"),
    ("###", "段落"),
    ("####", "子段落")
]

# 分段
splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
docs = splitter.split_text(text)

# 總段落數
print(f"共有{len(docs)}個段落")

# 每個段落內容
for i, d in enumerate(docs, 1):
  print(f"\n--- 段落 {i} ---")
  print("標題階層：", d.metadata)
  print("內容：", d.page_content,)

"""Embeddings"""

#將文本轉成向量
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

"""用Chroma建立向量庫

**Chroma**
"""

vectorstore = Chroma.from_documents(docs, embeddings, persist_directory="/content/drive/MyDrive/專題/程式碼專區/chroma_db_md")
vectorstore.persist()

"""將使用者問題轉成向量，找教材裡最相似的幾段內容

"""

def retrieve_docs_md(query, top_k=3):
  #載入Embeddings和Chroma向量庫
  embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
  vectorstore = Chroma(persist_directory="/content/drive/MyDrive/專題/程式碼專區/chroma_db_md", embedding_function=embeddings)

  results = vectorstore.similarity_search_with_score(query, k=top_k)
  related_texts = []

  for doc, score in results:
    #if score <= 1:
      related_texts.append((doc.page_content))

  return related_texts #回傳成一個list

"""測試"""

query = "一維陣列"
related_docs = retrieve_docs_md(query, top_k=3)

print("找到的相關教材段落：")
for i, doc in enumerate(related_docs, 1):
  print(f"{i}. {doc}\n")

"""**Chroma + BM25 混合搜尋**"""

def hybrid_search_md(query,top_k=5, weight_bm25=0.7, weight_vector=0.3):
    #載入Embeddings和Chroma向量庫
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vectorstore = Chroma(
      persist_directory="/content/drive/MyDrive/專題/程式碼專區/chroma_db_md",
      embedding_function=embeddings
    )

    #抓取所有文件（for BM25）
    all_docs = vectorstore.get(include=["documents"])["documents"]

    #準備BM25資料
    #把每份文件用split拆成單詞
    tokenized_corpus = [doc.split() for doc in all_docs]
    bm25 = BM25Okapi(tokenized_corpus)

    #BM25排序
    bm25_scores = bm25.get_scores(query.split())
    bm25_ranked = sorted(zip(bm25_scores, all_docs), key=lambda x: x[0], reverse=True)

    #取前20筆作為候選
    candidate_docs = [doc for _, doc in bm25_ranked[:20]]

    #做向量檢索
    query_emb = embeddings.embed_query(query) #把 query 轉成向量
    vector_results = []
    for doc in candidate_docs:
        doc_emb = embeddings.embed_query(doc)
        #餘弦相似度（越大越相似）
        score = np.dot(query_emb, doc_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(doc_emb))
        vector_results.append((doc, score))

    #取出BM25 數與向量分數
    bm25_scores_candidates = np.array([score for score, _ in bm25_ranked[:20]])
    vector_scores = np.array([score for _, score in vector_results])

    #標準化
    scaler = MinMaxScaler()
    bm25_scores_norm = scaler.fit_transform(bm25_scores_candidates.reshape(-1, 1)).flatten()
    vector_scores_norm = scaler.fit_transform(vector_scores.reshape(-1, 1)).flatten()

    #融合分數(70%來自BM25/30%來自向量相似度)
    final_scores = weight_bm25 * bm25_scores_norm + weight_vector * vector_scores_norm

    #排序
    sorted_pairs = sorted(zip(final_scores, candidate_docs), key=lambda x: x[0], reverse=True)
    sorted_results = [text for _, text in sorted_pairs]

    return sorted_results[:top_k]

"""測試"""

query = "一維陣列"
related_docs = hybrid_search_md(query,top_k=5)

print("找到的相關教材段落：")
for i, doc in enumerate(related_docs, 1):
  print(f"{i}. {doc}\n")

"""# MarkdownHeaderTextSplitter+ RecursiveCharacterTextSplitter方法讀取教材(/chroma_db_mdrc)"""

#載入教材PDF
loader = PyPDFLoader("/content/drive/MyDrive/專題/教材/CH3陣列_markdown.pdf")
pages = loader.load()

# 合併文字
text = "\n".join([p.page_content for p in pages])

# 定義標題層級
headers_to_split_on = [
    ("#", "章節"),
    ("##", "小節"),
    ("###", "段落"),
    ("####", "子段落")
]

#MarkdownHeaderTextSplitter分段
markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
docs = splitter.split_text(text)

#RecursiveCharacterTextSplitter分段
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,   # 每塊字數上限
    chunk_overlap=50, # 交疊避免斷句
    separators=["\n\n", "\n", "。", " "]  # 分割優先順序
)

final_docs = []
for doc in docs:
    sub_docs = text_splitter.split_text(doc.page_content)
    for sub_doc in sub_docs:
        final_docs.append(
            Document(
              page_content=sub_doc,
              metadata=doc.metadata  # 保留章節、小節資訊
          )
        )

# 總段落數
print(f"共有{len(final_docs)}個段落")

# 每個段落內容
for i, d in enumerate(final_docs, 1):
  print(f"\n--- 段落 {i} ---")
  print("標題階層：", d.metadata)
  print("內容：", d.page_content)

"""Embeddings"""

#將文本轉成向量
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

"""用Chroma建立向量庫"""

vectorstore = Chroma.from_documents(final_docs, embeddings, persist_directory="/content/drive/MyDrive/專題/程式碼專區/chroma_db_mdrc")
vectorstore.persist()

"""**Chroma**

將使用者問題轉成向量，找教材裡最相似的幾段內容
"""

def retrieve_docs_mdrc(query, top_k=5):
  #載入Embeddings和Chroma向量庫
  embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
  vectorstore = Chroma(persist_directory="/content/drive/MyDrive/專題/程式碼專區/chroma_db_mdrc", embedding_function=embeddings)

  #取得關鍵字
  query_text = " ".join(query.get("keywords", []))

  results = vectorstore.similarity_search_with_score(query_text, k=top_k)
  related_texts = []

  for doc, score in results:
    #if score <= 1:
      related_texts.append((doc.page_content))

  return related_texts #回傳成一個list

"""測試"""

query = {"keywords": ["一維陣列", "索引"]}
related_docs = retrieve_docs(query, top_k=5)

print("找到的相關教材段落：")
for i, doc in enumerate(related_docs, 1):
  print(f"{i}. {doc}\n")

"""**Chroma + BM25 混合搜尋**"""

def retrieve_docs(query, top_k=5, weight_bm25=0.7, weight_vector=0.3, k_bm25=20):
  # 載入 Embeddings 和 Chroma
  embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
  vectorstore = Chroma(
      persist_directory="/content/drive/MyDrive/專題/程式碼專區/chroma_db_mdrc",
      embedding_function=embeddings
  )

  #取得關鍵字
  query_text = " ".join(query.get("keywords", []))

  # 取得所有段落文字
  all_docs = vectorstore.get(include=["documents"])["documents"]

  # BM25 準備
  tokenized_corpus = [doc.split() for doc in all_docs]
  bm25 = BM25Okapi(tokenized_corpus)

  # BM25 排序，取前 k_bm25 作為候選
  bm25_scores = bm25.get_scores(query_text.split())
  bm25_ranked = sorted(zip(bm25_scores, all_docs), key=lambda x: x[0], reverse=True)
  candidate_docs = [doc for _, doc in bm25_ranked[:k_bm25]]

  # 向量檢索
  query_emb = embeddings.embed_query(query_text)
  vector_results = []
  for doc in candidate_docs:
      doc_emb = embeddings.embed_documents([doc])[0]
      score = np.dot(query_emb, doc_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(doc_emb))
      vector_results.append((doc, score))

  # 標準化並融合分數
  bm25_scores_candidates = np.array([score for score, _ in bm25_ranked[:k_bm25]])
  vector_scores = np.array([score for _, score in vector_results])
  scaler = MinMaxScaler()
  bm25_scores_norm = scaler.fit_transform(bm25_scores_candidates.reshape(-1,1)).flatten()
  vector_scores_norm = scaler.fit_transform(vector_scores.reshape(-1,1)).flatten()

  final_scores = weight_bm25 * bm25_scores_norm + weight_vector * vector_scores_norm

  # 排序回傳
  sorted_pairs = sorted(zip(final_scores, candidate_docs), key=lambda x: x[0], reverse=True)
  sorted_results = [text for _, text in sorted_pairs]

  return sorted_results[:top_k]

"""測試"""

query = {"keywords": ["一維陣列", "二維陣列"]}
related_docs = hybrid_search(query,top_k=5)

print("找到的相關教材段落：")
for i, doc in enumerate(related_docs, 1):
  print(f"{i}. {doc}\n")

"""# 單元教材提供

回傳教材內容
"""

docs_dict = {}

for doc in docs:
  text = doc.page_content.strip()
  meta = doc.metadata

  # 從 metadata 取「章節」當作單元代號
  chapter_title = meta.get("章節")
  if chapter_title:
    #轉成 '3.1' 格式
    match = re.match(r"(\d+)-(\d+)", chapter_title)
    if match:
      unit_code = f"{match.group(1)}.{match.group(2)}"
    else:
      unit_code = chapter_title  #如果沒對應到，保留原樣

    if unit_code not in docs_dict:
        docs_dict[unit_code] = []
    docs_dict[unit_code].append(text)

for unit_code, paragraphs in docs_dict.items():
    print(f"單元編號: {unit_code}")

    for p in paragraphs:
        print(p)
    print("-" * 50)  # 分隔線

#number:使用者輸入單元編號(1,2,3...)
#docs_dict: key = 單元編號, value = list of 段落文字
def get_unit(number):

  #建立docs_dict
  docs_dict = {}

  for doc in docs:
    text = doc.page_content.strip() #取出文字內容
    meta = doc.metadata #取出標題

    #從metadata取章節
    chapter_title = meta.get("章節")
    if chapter_title:
      #轉成 '3.1' 格式
      match = re.match(r"(\d+)-(\d+)", chapter_title)
      if match:
        unit_code = f"{match.group(1)}.{match.group(2)}"
      else:
        unit_code = chapter_title  #如果沒對應到，保留原樣

      if unit_code not in docs_dict:
          docs_dict[unit_code] = []
      docs_dict[unit_code].append(text)

  #單元編號
  unit_map = {
    1: "3.1",
    2: "3.2",
    3: "3.3",
    4: "3.4",
  }

  #根據number從unit_map取出對應的單元編號
  unit_code = unit_map.get(number)
  if not unit_code:
    return "找不到對應單元"

  combined = []

  #key:單元編號 / value:該單元段落列表
  for key, paragraphs in docs_dict.items():
    #加入子單元
    if key.startswith(unit_code + ".") or key == unit_code:
      combined.append(f"=== {key} ===") #加上單元
      combined.extend(paragraphs) #合併列表
  if combined:
    return "\n".join(combined)
  else:
    return "找不到該單元教材"

print(get_unit(4))