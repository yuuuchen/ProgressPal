# -*- coding: utf-8 -*-
"""rag_0815.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14jAxpxWKDhlFS2xe0hTOhfQiZ7txP5-P
"""
from langchain_community.document_loaders import PyPDFLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter,MarkdownHeaderTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import ConversationalRetrievalChain
from sentence_transformers import SentenceTransformer
from langchain.schema import Document
import re
from rank_bm25 import BM25Okapi
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import os

"""測試

# **教材向量庫**

建立向量庫
"""

#讀取資料夾裡的所有.md檔案
md_files = [f for f in os.listdir("/content/drive/MyDrive/專題/教材") if f.endswith(".md")]

all_docs = []

#定義標題層級
headers_to_split_on = [
    ("#", "章節"),
    ("##", "單元"),
    ("###", "段落"),
    ("####", "子段落")
]
markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)

for file in md_files:
  file_path = os.path.join("/content/drive/MyDrive/專題/教材", file)

  #讀取檔案
  loader = TextLoader(file_path, encoding="utf-8")
  docs = loader.load()

  for d in docs:
    #MarkdownHeaderTextSplitter分段
    md_header_splits = markdown_splitter.split_text(d.page_content)

    for doc in md_header_splits:
      header = doc.metadata.get("段落", "")
      content_with_header = f"{header}\n{doc.page_content}" if header else doc.page_content

      all_docs.append(
        Document(
          page_content=content_with_header,
          metadata={
              **doc.metadata,   # 保留章節、小節資訊
              "source": file    # 加上檔名來源
          }
        )
      )

print(f"總共有 {len(all_docs)} 個段落，來自 {len(md_files)} 個 Markdown 檔案")

#將文本轉為向量
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

#存入 Chroma 向量資料庫
vectorstore = Chroma.from_documents(
    documents=all_docs,
    embedding=embeddings,
    persist_directory="/content/drive/MyDrive/專題/教材/teaching_material"
)

vectorstore.persist()

for i, doc in enumerate(all_docs[:50], 1):
    print(f"--- 段落 {i} ---")
    print(doc.page_content.strip())   # 段落文字
    print("Metadata:", doc.metadata)  # 對應的標題資訊（章節/單元/來源檔案）
    print()

"""**回傳教材內容**"""

#排序單元編號
def parse_unit_code(unit_code):
    parts = unit_code.split("-")
    nums = []
    for p in parts[:2]:  # 只取前兩項：章節與單元
        match = re.match(r"(\d+)", p)
        if match:
            nums.append(int(match.group(1)))
        else:
            nums.append(0)
    return nums  #回傳一個數字列表，例如 "1-1-1" → [1,1]

#依據章節與單元號碼，輸出該單元的教材內容字串
def get_unit(chapter, unit):
  docs_dict = {}

  for doc in all_docs:
    text = doc.page_content.strip()
    meta = doc.metadata

    unit_title = meta.get("單元", "")
    paragraph = meta.get("段落", "")
    if unit_title:
        unit_num = re.match(r"(\d+-\d+)", unit_title).group(0)  # 抓單元開頭數字

        if unit_num not in docs_dict:
            docs_dict[unit_num] = []

        # 在內容中保留段落資訊
        docs_dict[unit_num].append(f"[{paragraph}] - {text}")

  target_code = f"{chapter}-{unit}" #組合成目標單元編號

  combined = []
  for key in sorted(docs_dict.keys(), key=parse_unit_code):
    if key == target_code:
      combined.append(f"=== {key} ===")
      combined.extend(docs_dict[key])

  if combined:
    return "\n".join(combined)
  else:
    return "找不到該單元教材"

"""測試"""

print(get_unit(4,3))

"""**Chroma + BM25 混合搜尋**"""

def retrieve_docs(query, top_k=5, weight_bm25=0.7, weight_vector=0.3):
  #載入Embeddings和Chroma
  embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
  vectorstore = Chroma(
      persist_directory="/content/drive/MyDrive/專題/程式碼專區/teaching_material",
      embedding_function=embeddings
  )

  #取得所有段落文字
  all_texts = [doc.page_content for doc in all_docs]

  #BM25準備
  tokenized_corpus = [doc.split() for doc in all_texts]
  bm25 = BM25Okapi(tokenized_corpus)

  #取得關鍵字
  keywords = query.get("keywords", [])
  results = []

  #每個關鍵字單獨檢索
  for keyword in keywords:
    query_text = keyword

    #BM25排序，取前20筆作為候選
    bm25_scores = bm25.get_scores(query_text.split())
    bm25_ranked = sorted(zip(bm25_scores, all_texts), key=lambda x: x[0], reverse=True)
    candidate_docs = [doc for _, doc in bm25_ranked[:20]]

    #向量檢索
    query_emb = embeddings.embed_query(query_text)
    vector_results = []
    for doc in candidate_docs:
        doc_emb = embeddings.embed_documents([doc])[0]
        score = np.dot(query_emb, doc_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(doc_emb))
        vector_results.append((doc, score))

    #標準化並融合分數
    bm25_scores_candidates = np.array([score for score, _ in bm25_ranked[:20]])
    vector_scores = np.array([score for _, score in vector_results])
    scaler = MinMaxScaler()
    bm25_scores_norm = scaler.fit_transform(bm25_scores_candidates.reshape(-1,1)).flatten()
    vector_scores_norm = scaler.fit_transform(vector_scores.reshape(-1,1)).flatten()

    final_scores = weight_bm25 * bm25_scores_norm + weight_vector * vector_scores_norm

    #排序回傳
    sorted_pairs = sorted(zip(final_scores, candidate_docs), key=lambda x: x[0], reverse=True)
    sorted_results = [text for _, text in sorted_pairs]

    #去除重複段落
    sorted_results = list(dict.fromkeys(sorted_results))

    results.extend(sorted_results[:top_k])

  #去除重複段落
  results = list(dict.fromkeys(results))

  return results

query = {"keywords": ["鏈結串列","陣列","堆疊"]}
related_docs = retrieve_docs(query,5)

print("找到的相關教材段落：")
for i, doc in enumerate(related_docs, 1):
  print(f"{i}. {doc}\n")
