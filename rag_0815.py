# -*- coding: utf-8 -*-
"""rag_0815.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14jAxpxWKDhlFS2xe0hTOhfQiZ7txP5-P
"""


from langchain_community.document_loaders import PyPDFLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter,MarkdownHeaderTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import ConversationalRetrievalChain
from sentence_transformers import SentenceTransformer
from langchain.schema import Document

"""# RecursiveCharacterTextSplitter方法讀取教材(/chroma_db)

Chunk 切分
"""

#載入教材PDF
loader = PyPDFLoader("/content/drive/MyDrive/專題/教材/CH3陣列_整理版.pdf")
documents = loader.load()
#切分文字
text_splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=50)
docs = text_splitter.split_documents(documents)

print(f"共分成 {len(docs)} 段")

# for i, chunk in enumerate(docs, 1):
#   print(f"段落 {i}:\n{chunk.page_content}\n{'-'*50}")

"""Embeddings"""

#將文本轉成向量
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

"""用Chroma建立向量庫"""

vectorstore = Chroma.from_documents(docs, embeddings, persist_directory="/content/drive/MyDrive/專題/程式碼專區/chroma_db")
vectorstore.persist()

"""將使用者問題轉成向量，找教材裡最相似的幾段內容

**Chroma**
"""

def retrieve_docs_rc(query, top_k):
  #載入Embeddings和Chroma向量庫
  embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
  vectorstore = Chroma(persist_directory="/content/drive/MyDrive/專題/程式碼專區/chroma_db", embedding_function=embeddings)

  results = vectorstore.similarity_search_with_score(query, k=top_k)
  related_texts = []

  for doc, score in results:
    if score <= 1:
      related_texts.append((doc.page_content))

  return related_texts #回傳成一個list

"""測試"""

query = "二維陣列是什麼?"
related_docs = retrieve_docs_rc(query, top_k=5)

print("找到的相關教材段落：")
for i, doc in enumerate(related_docs, 1):
  print(f"{i}. {doc}\n")

"""**Chroma + BM25 混合搜尋**"""


from rank_bm25 import BM25Okapi
from sklearn.preprocessing import MinMaxScaler
import numpy as np

def hybrid_search(query, k_vector=20, top_k=5, weight_vector=0.7, weight_bm25=0.3):
    #載入Embeddings和Chroma向量庫
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vectorstore = Chroma(persist_directory="/content/drive/MyDrive/專題/程式碼專區/chroma_db", embedding_function=embeddings)

    #向量檢索
    vector_results = vectorstore.similarity_search_with_score(query, k=k_vector)

    if not vector_results:
        return []

    #準備BM25資料
    docs_text = [doc.page_content for doc, _ in vector_results]
    bm25 = BM25Okapi([t.split() for t in docs_text])
    bm25_scores = bm25.get_scores(query.split())

    #向量分數（距離轉為相似度1 - score）
    vector_scores = np.array([1 - score for _, score in vector_results])

    #標準化分數
    scaler = MinMaxScaler()
    vector_scores_norm = scaler.fit_transform(vector_scores.reshape(-1, 1)).flatten()
    bm25_scores_norm = scaler.fit_transform(np.array(bm25_scores).reshape(-1, 1)).flatten()

    #加權融合
    final_scores = weight_vector * vector_scores_norm + weight_bm25 * bm25_scores_norm

    #根據融合分數排序
    sorted_pairs = sorted(zip(final_scores, docs_text), key=lambda x: x[0], reverse=True)

    #放進list
    sorted_results = [text for _, text in sorted_pairs]

    #回傳前top_k筆的list
    return sorted_results[:top_k]

"""測試"""

query = "二維陣列是什麼?"
related_docs = hybrid_search(query, k_vector=20, top_k=5)

print("找到的相關教材段落：")
for i, doc in enumerate(related_docs, 1):
  print(f"{i}. {doc}\n")

"""# MarkdownHeaderTextSplitter方法讀取教材(/chroma_db_md)

Chunk 切分
"""

#載入教材PDF
loader = PyPDFLoader("/content/drive/MyDrive/專題/教材/CH3陣列_markdown.pdf")
pages = loader.load()

# 合併文字
text = "\n".join([p.page_content for p in pages])

# 定義標題層級
headers_to_split_on = [
    ("#", "章節"),
    ("##", "小節"),
    ("###", "段落"),
    ("####", "子段落")
]

# 分段
splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
docs = splitter.split_text(text)

# 總段落數
print(f"共有{len(docs)}個段落")

# 每個段落內容
for i, d in enumerate(docs, 1):
  print(f"\n--- 段落 {i} ---")
  print("標題階層：", d.metadata)
  print("內容：", d.page_content,)

"""Embeddings"""

#將文本轉成向量
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

"""用Chroma建立向量庫

**Chroma**
"""

vectorstore = Chroma.from_documents(docs, embeddings, persist_directory="/content/drive/MyDrive/專題/程式碼專區/chroma_db_md")
vectorstore.persist()

"""將使用者問題轉成向量，找教材裡最相似的幾段內容

"""

def retrieve_docs_md(query, top_k=3):
  #載入Embeddings和Chroma向量庫
  embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
  vectorstore = Chroma(persist_directory="/content/drive/MyDrive/專題/程式碼專區/chroma_db_md", embedding_function=embeddings)

  results = vectorstore.similarity_search_with_score(query, k=top_k)
  related_texts = []

  for doc, score in results:
    #if score <= 1:
      related_texts.append((doc.page_content))

  return related_texts #回傳成一個list

"""測試"""

query = "二維陣列是甚麼"
related_docs = retrieve_docs_md(query, top_k=3)

print("找到的相關教材段落：")
for i, doc in enumerate(related_docs, 1):
  print(f"{i}. {doc}\n")

"""# MarkdownHeaderTextSplitter+ RecursiveCharacterTextSplitter方法讀取教材(/chroma_db_mdrc)"""

#載入教材PDF
loader = PyPDFLoader("/content/drive/MyDrive/專題/教材/CH3陣列_markdown.pdf")
pages = loader.load()

# 合併文字
text = "\n".join([p.page_content for p in pages])

# 定義標題層級
headers_to_split_on = [
    ("#", "章節"),
    ("##", "小節"),
    ("###", "段落"),
    ("####", "子段落")
]

#MarkdownHeaderTextSplitter分段
markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
docs = splitter.split_text(text)

#RecursiveCharacterTextSplitter分段
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=250,   # 每塊字數上限
    chunk_overlap=50, # 交疊避免斷句
    separators=["\n\n", "\n", "。", " "]  # 分割優先順序
)

final_docs = []
for doc in docs:
    sub_docs = text_splitter.split_text(doc.page_content)
    for sub_doc in sub_docs:
        final_docs.append(
            Document(
              page_content=sub_doc,
              metadata=doc.metadata  # 保留章節、小節資訊
          )
        )

# 總段落數
print(f"共有{len(final_docs)}個段落")

# 每個段落內容
for i, d in enumerate(final_docs, 1):
  print(f"\n--- 段落 {i} ---")
  print("標題階層：", d.metadata)
  print("內容：", d.page_content)

"""Embeddings"""

#將文本轉成向量
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

"""用Chroma建立向量庫

**Chroma**
"""

vectorstore = Chroma.from_documents(final_docs, embeddings, persist_directory="/content/drive/MyDrive/專題/程式碼專區/chroma_db_mdrc")
vectorstore.persist()

"""將使用者問題轉成向量，找教材裡最相似的幾段內容

"""

def retrieve_docs(query, top_k=3):
  #載入Embeddings和Chroma向量庫
  embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
  vectorstore = Chroma(persist_directory="/content/drive/MyDrive/專題/程式碼專區/chroma_db_mdrc", embedding_function=embeddings)

  results = vectorstore.similarity_search_with_score(query, k=top_k)
  related_texts = []

  for doc, score in results:
    #if score <= 1:
      related_texts.append((doc.page_content))

  return related_texts #回傳成一個list

"""測試"""

query = "一維陣列和二維陣列差別"
related_docs = retrieve_docs(query, top_k=5)

print("找到的相關教材段落：")
for i, doc in enumerate(related_docs, 1):
  print(f"{i}. {doc}\n")

"""# 單元教材提供

回傳教材內容
"""
import re

docs_dict = {}

for doc in docs:
  text = doc.page_content.strip()
  meta = doc.metadata

  # 從 metadata 取「章節」當作單元代號
  chapter_title = meta.get("章節")
  if chapter_title:
    #轉成 '3.1' 格式
    match = re.match(r"(\d+)-(\d+)", chapter_title)
    if match:
      unit_code = f"{match.group(1)}.{match.group(2)}"
    else:
      unit_code = chapter_title  #如果沒對應到，保留原樣

    if unit_code not in docs_dict:
        docs_dict[unit_code] = []
    docs_dict[unit_code].append(text)

for unit_code, paragraphs in docs_dict.items():
    print(f"單元編號: {unit_code}")

    for p in paragraphs:
        print(p)
    print("-" * 50)  # 分隔線

import re

#number:使用者輸入單元編號(1,2,3...)
#docs_dict: key = 單元編號, value = list of 段落文字
def get_unit(number):

  #建立docs_dict
  docs_dict = {}

  for doc in docs:
    text = doc.page_content.strip() #取出文字內容
    meta = doc.metadata #取出標題

    #從metadata取章節
    chapter_title = meta.get("章節")
    if chapter_title:
      #轉成 '3.1' 格式
      match = re.match(r"(\d+)-(\d+)", chapter_title)
      if match:
        unit_code = f"{match.group(1)}.{match.group(2)}"
      else:
        unit_code = chapter_title  #如果沒對應到，保留原樣

      if unit_code not in docs_dict:
          docs_dict[unit_code] = []
      docs_dict[unit_code].append(text)

  #單元編號
  unit_map = {
    1: "3.1",
    2: "3.2",
    3: "3.3",
    4: "3.4",
  }

  #根據number從unit_map取出對應的單元編號
  unit_code = unit_map.get(number)
  if not unit_code:
    return "找不到對應單元"

  combined = []

  #key:單元編號 / value:該單元段落列表
  for key, paragraphs in docs_dict.items():
    #加入子單元
    if key.startswith(unit_code + ".") or key == unit_code:
      combined.append(f"=== {key} ===") #加上單元
      combined.extend(paragraphs) #合併列表
  if combined:
    return "\n".join(combined)
  else:
    return "找不到該單元教材"

print(get_unit(4))
